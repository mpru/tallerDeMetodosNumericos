---
title: "Unidad 3"
subtitle: "Solución de Sistemas de Ecuaciones Lineales"
author: 
    - "Cecilia Rapelli"
    - "Marcos Prunello"
date: "Año 2019"
output: 
    beamer_presentation:
      includes:
        in_header: modify_header.txt
      theme: Madrid
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, comment=NA, fig.align = "center", message = F)
# setwd("~/gdrive/MetodosNumericos/CodigoR")
```

# Introducción

- **Objetivo**: examinar los aspectos numéricos que se presentan al resolver sistemas de ecuaciones lineales de la forma:

$$
\begin{cases} 
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\
\vdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = b_n
\end{cases}
$$

- $n$ ecuaciones, $n$ incógnitas: sistema de orden $n \times n$.
- Los coeficientes $a_{ij}$ y los términos independientes $b_i$ son reales fijos.

# Repaso

- Representación matricial: $\mathbf{Ax=b}$, de dimensión $n\times n$, $n\times 1$ y $n\times 1$, respectivamente. 

$$
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn} 
\end{bmatrix}
\times 
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}
$$

- Llamamos **matriz ampliada** o **aumentada** a:

$$
\begin{bmatrix}
	\mathbf{A} & \mathbf{b}
\end{bmatrix}
=
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} & b_1\\
a_{21} & a_{22} & \cdots & a_{2n} & b_2\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
a_{n1} & a_{n2} & \cdots & a_{nn} & b_n 
\end{bmatrix}
$$

# Repaso

- Un sistema de ecuaciones lineales se clasifica en:

    - **Compatible determinado**: tiene una única solución
    - **Compatible indeterminado**: tiene infinitas soluciones
    - **Incompatible**: no existe solución
 
<!--  
# Repaso

- El **rango** ($r$) de una matriz es el orden de la mayor submatriz cuadrada cuyo determinante es distinto de 0.
- **Teorema de rangos**:

    - $r(\mathbf{A}) < r([\mathbf{A} \quad \mathbf{b}]) \implies$ sistema incompatible
    - $r(\mathbf{A}) = r([\mathbf{A} \quad \mathbf{b}]) \implies$ sistema compatible
    
        - $r(\mathbf{A}) = n \implies$ determinado
        - $r(\mathbf{A}) < n \implies$ indeterminado
-->

# Repaso

**Teorema**. Las siguientes condiciones son equivalentes:

- El sistema $\mathbf{Ax=b}$ tiene solución única (es compatible determinado).
- La matriz $\mathbf{A}$ es invertible (existe $\mathbf{A^{-1}}$).
- La matriz $\mathbf{A}$ es no singular ($\det \mathbf{A} \neq 0$).
- El sistema $\mathbf{Ax=0}$ tiene como única solución $\mathbf{x=0}$.

# Repaso

- Dos sistemas de orden $n \times n$ son **equivalentes** si tienen el mismo conjunto de soluciones.
- Existen ciertas transformaciones sobre las ecuaciones de un sistema que no cambian el conjunto de soluciones (producen un sistema equivalente):

    - **Intercambio**: el orden de las ecuaciones puede cambiarse.
    - **Escalado**: multiplicar una ecuación por una constante no nula.
    - **Sustitución**: una ecuación puede ser reemplazada por una combinación lineal de las ecuaciones del sistema (*teorema fundamental de la equivalencia*)

- Realizar estas transformaciones sobre las ecuaciones es equivalente a realizar las mismas operaciones sobre las filas de la matriz aumentada.

# Notación

- Notación que ayuda a expresar algoritmos con operaciones matriciales y facilita la escritura de los programas en IML o R.

- Dada una matriz $\mathbf{Z}$ de dimensión $n \times m$, anotamos:

    - $z_{ij} = \mathbf{Z}[i, j]$: elemento en la fila $i$ y columna $j$ de la matriz $\mathbf{Z}$
    - $\mathbf{Z}[i,]$: vector fila de dimensión $1\times m$ constituido por la $i$-ésima fila de la matriz $\mathbf{Z}$
    - $\mathbf{Z}[,j]$: vector columna $n\times 1$ constituido por la $j$-ésima columna de la matriz $\mathbf{Z}$
    - $\mathbf{Z}[i,k:l]$: matriz de dimensión $1\times (l-k+1)$ constituida con los elementos $z_{i,k}, z_{i,k+1}, \cdots, z_{i,l}$ de la matriz $\mathbf{Z}$, $l \geq k$.
    - $\mathbf{Z}[c:d,k:l]$: matriz de dimensión $(d-c+1)\times (l-k+1)$ constituida por la submatriz que contiene las filas de $\mathbf{Z}$ desde la $c$ hasta $d$ y las columnas de $\mathbf{Z}$ desde la $k$ hasta la $l$, $d \geq c$, $l \geq k$.

# Notación

- Dado un vector $\mathbf{Z}$ de largo $n$, anotamos:

    - $\mathbf{Z}[i]$: elemento $i$-ésimo del vector $\mathbf{Z}$
    - $\mathbf{Z}[k:l]$: vector de largo $(l-k+1)$ constituido con los elementos $z_{k}, z_{k+1}, \cdots, z_{l}$ del vector $\mathbf{Z}$, $l \geq k$.

# Métodos de Resolución de Sistemas de Ecuaciones

- **Métodos exactos**: permiten obtener la solución del sistema de manera directa.

	- Método de Eliminación de Gauss
	- Método de Gauss-Jordan
	
- **Métodos aproximados**: utilizan algoritmos iterativos que calculan las solución del sistema por aproximaciones sucesivas.

	- Método de Jacobi
	- Método de Gauss-Seidel
	
- En muchas ocasiones los métodos aproximados permiten obtener un grado de exactitud superior al que se puede obtener empleando los denominados métodos exactos, debido fundamentalmente a los errores de truncamiento que se producen en el proceso.

# Sistemas fáciles de resolver

**Caso 1: La matriz** $\mathbf{A}$ es **diagonal**

$$
\begin{bmatrix}
a_{11} & 0 & \cdots & 0 \\
0 & a_{22} & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & a_{nn} 
\end{bmatrix}
\times 
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}
$$

- El sistema se reduce a $n$ ecuaciones simples y la solución es: 

\setlength{\abovedisplayshortskip}{-10pt}
\setlength{\belowdisplayshortskip}{-10pt}
$$
\mathbf{x} =
\begin{bmatrix}
	b_1/a_{11} \\ b_2/a_{22} \\ \vdots \\ b_n/a_{nn}
\end{bmatrix}
$$

# Sistemas fáciles de resolver

**Caso 2: La matrix** $\mathbf{A}$ **es triangular superior**:

\setlength{\abovedisplayshortskip}{-5pt}
\setlength{\belowdisplayshortskip}{-5pt}
$$
\begin{bmatrix}
a_{11} & a_{12} & a_{13} & \cdots & a_{1n} \\
0 & a_{22} & a_{23} & \cdots & a_{2n} \\
0 & 0 & a_{33} & \cdots & a_{3n} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & a_{nn}
\end{bmatrix}
\times 
\begin{bmatrix}
x_1 \\ x_2 \\ \vdots \\ x_n
\end{bmatrix}
=
\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_n
\end{bmatrix}
$$

- La solución de $x_n$ es inmediata y a partir de ella se encuentran las restantes siguiendo el orden inverso $x_{n-1}, \cdots, x_1$, aplicando el algoritmo de **sustitución regresiva**:

\setlength{\abovedisplayshortskip}{-5pt}
\setlength{\belowdisplayshortskip}{-5pt}
$$
x_n = \frac{b_n}{a_{nn}} \text{ y } x_k = \frac{b_k - \sum_{j = k+1}^{n}a_{kj}x_j}{a_{kk}} \quad k = n-1, n-2, \cdots, 1
$$

# Sistemas fáciles de resolver

**Caso 2: La matrix** $\mathbf{A}$ **es triangular superior**:

- Empleando la notación matricial vista, la suma en el algoritmo puede reescribirse con productos matriciales:

$$
\mathbf{x}[n] = \frac{\mathbf{b}[n]}{\mathbf{A}[n,n]} \qquad \mathbf{x}[k] = \frac{\mathbf{b}[k] - \mathbf{A}[k, k+1 : n] \times \mathbf{x}[k+1 : n]}{\mathbf{A}[k,k]}
$$
$$
\quad k = n-1, n-2, \cdots, 1
$$

- Iniciando el vector solución con ceros ($\mathbf{x} = [0 ~ 0 \cdots 0]$), la expresión anterior se simplifica en:

\setlength{\abovedisplayskip}{0pt}
\setlength{\belowdisplayskip}{0pt}

$$
\mathbf{x}[n] = \frac{\mathbf{b}[n]}{\mathbf{A}[n,n]} \qquad \mathbf{x}[k] = \frac{\mathbf{b}[k] - \mathbf{A}[k, ] \times \mathbf{x}}{\mathbf{A}[k,k]}
\qquad k = n-1, n-2, \cdots, 1
$$


- Ver el algoritmo y programarlo.

# Sistemas fáciles de resolver

**Caso 2: La matrix** $\mathbf{A}$ **es triangular inferior**:

- Obtención de la solución análoga al caso anterior.

**Observación**

- En los casos anteriores asumimos $a_{kk} \neq 0 \forall k$. De lo contrario el sistema no tiene solución o tiene infinitas.
- Recordar que un sistema lineal $\mathbf{Ax=b}$ tiene solución única si y sólo si $\det \mathbf{A} \neq 0$ y que si un elemento de la diagonal principal de una matriz triangular es cero, entonces $\det \mathbf{A} = 0$.

# Eliminación gaussiana

- Es un método para resolver un sistema lineal general $\mathbf{Ax=b}$ de $n$ ecuaciones con $n$ incógnitas.
- El objetivo es construir un sistema equivalente donde la matriz de coeficientes sea triangular superior para obtener las soluciones con el algoritmo de sustitución regresiva.
- El método consiste en ir eliminando incógnitas en las ecuaciones de manera sucesiva.

# Eliminación gaussiana

- Ejemplo: resolver el siguiente sistema

$$
\begin{cases} 
x+2y-z+3t=-8 \\
2x+2z-t=13 \\
-x+y+z-t=8\\
3x+3y-z+2t = -1
\end{cases}
\mathbf{A}=
\begin{bmatrix}
1 & 2 & -1 & 3 \\
2 & 0 & 2 & -1 \\
-1 & 1 & 1 & -1 \\
3 & 3 & -1 & 2 &  
\end{bmatrix}
\mathbf{x} = 
\begin{bmatrix}
x \\ y \\ z \\ t
\end{bmatrix}
\mathbf{b} =
\begin{bmatrix}
-8 \\ 13 \\ 8 \\ -1
\end{bmatrix}
$$

- Matriz aumentada:

$$
\begin{bmatrix}
	\mathbf{A} & \mathbf{b}
\end{bmatrix}
=
\begin{bmatrix}
	1 & 2 & -1 & 3 &\aug& -8\\
	2 & 0 & 2 & -1 &\aug& 13\\
	-1 & 1 & 1 & -1 &\aug& 8\\
	3 & 3 & -1 & 2 &\aug& -1  
\end{bmatrix}
$$

# Eliminación gaussiana

- En el primer paso eliminamos la incógnita $x$ en las ecuaciones 2, 3 y 4, buscando que queden ceros en toda la primera columna excepto en el elemento diagonal.
- La eliminación se hace empleando las transformaciones que producen un sistema equivalente.
- Observando con detenimiento, esto se puede lograr realizando los siguientes reemplazos:

$$
\begin{array}{cl}
\text{Fila 2} &= \text{ Fila 2} - 2 \times \text{Fila 1} \\ 
\text{Fila 3} &= \text{ Fila 3} - (-1) \times \text{Fila 1} \\
\text{Fila 4} &= \text{ Fila 4} - 3 \times \text{Fila 1} 
\end{array}
$$

# Eliminación gaussiana

- A la Fila 1 se le dice **fila pivote** y al elemento $a_{11}=1$, **pivote**.
- A los valores $2$, $-1$ y $3$ que multiplican a la fila pivote en los reemplazos se les dice **multiplicadores**.
- Los multiplicadores se simbolizan y obtienen con: $m_{r1} = a_{r1} / a_{11}, \quad r = 2, 3, 4$.
- De esta forma, los reemplazos realizados se generalizan como: Fila $r$ = Fila $r$ - $m_{r1}\times$ Fila 1.
- El resultado de los reemplazos anteriores es:

$$
\begin{matrix}
\text{pivote} \rightarrow \\ m_{21} = 2 \\ m_{31} = -1 \\ m_{41} = 3
\end{matrix}
\begin{bmatrix}
1 & 2 & -1 & 3 &\aug& -8\\
2 & 0 & 2 & -1 &\aug& 13\\
-1 & 1 & 1 & -1 &\aug& 8\\
3 & 3 & -1 & 2 &\aug& -1  
\end{bmatrix}
\implies
\begin{bmatrix}
1 & 2 & -1 & 3 &\aug& -8\\
0 & -4 & 4 & -7 &\aug& 29\\
0 & 3 & 0 & 2 &\aug& 0\\
0 & -3 & 2 & -7 &\aug& 23  
\end{bmatrix}
$$

# Eliminación gaussiana

- En el segundo paso, eliminamos la incógnita $y$ en las ecuaciones 3 y 4.
- La fila pivote pasa a ser la segunda y el pivote es $a_{22}=-4$.
- Los multiplicadores son $m_{r2}=a_{r2}/a_{22}$, $r=3,4$, dando lugar a los reemplazos:

$$
\begin{array}{cl}
\text{Fila 3} &= \text{ Fila 3} - (-3/4) \times \text{Fila 2} \\
\text{Fila 4} &= \text{ Fila 4} - 3/4 \times \text{Fila 2} 
\end{array}
$$

- El resultado es:

$$
\begin{matrix}
 \\ \text{pivote} \rightarrow \\ m_{32} = -3/4 \\ m_{42} = 3/4
\end{matrix}
\begin{bmatrix}
1 & 2 & -1 & 3 &\aug& -8\\
0 & -4 & 4 & -7 &\aug& 29\\
0 & 3 & 0 & 2 &\aug& 0\\
0 & -3 & 2 & -7 &\aug& 23  
\end{bmatrix}
\implies
\begin{bmatrix}
1 & 2 & -1 & 3 &\aug& -8\\
0 & -4 & 4 & -7 &\aug& 29\\
0 & 0 & -12 & 13 &\aug& -87\\
0 & 0 & 4 & 7 &\aug& -5  
\end{bmatrix}
$$

# Eliminación gaussiana

- Finalmente, eliminamos la incógnita $z$ en la última ecuación.
- La fila pivote es la 3º y el pivote es $a_{33}=-12$.
- El multiplicador es $m_{43}=a_{43}/a_{33}=-4/12$.
- El reemplazo a realizar es Fila 4 = Fila 4 - $(-4/12)\times$ Fila 3.
- El resultado es:

$$
\hspace{-0.25cm}
\begin{matrix}
\\ \\ \text{pivote} \rightarrow \\ m_{43} = -4/12
\end{matrix}
\begin{bmatrix}
1 & 2 & -1 & 3 &\aug& -8\\
0 & -4 & 4 & -7 &\aug& 29\\
0 & 0 & -12 & 13 &\aug& -87\\
0 & 0 & 4 & 7 &\aug& -5  
\end{bmatrix}
\implies
\begin{bmatrix}
1 & 2 & -1 & 3 &\aug& -8\\
0 & -4 & 4 & -7 &\aug& 29\\
0 & 0 & -12 & 13 &\aug& -87\\
0 & 0 & 0 & -136 &\aug& 408  
\end{bmatrix}
$$

# Eliminación gaussiana

- Hemos llegado a un sistema equivalente cuya matriz de coeficientes es triangular superior, en el que aplicamos el algoritmo de sustitución regresiva:

$$
\left\{
\begin{aligned}
x+2y-z-3t &=-8 \cr
-4y+4z-7t &=29\cr
-12z+13t &=-87\cr
-136t &=408
\end{aligned}
\right.
\implies
\left\{
\begin{aligned}
x &= 1\\ y &= 2\\ z &= 4\\ t &= -3
\end{aligned}
\right.
$$

# Eliminación gaussiana con pivoteo trivial

- **Observación**: en el algoritmo visto es necesario que los pivotes $a_{qq} \neq 0 ~\forall q$.
- Si en uno de los pasos encontramos un $a_{qq} = 0$, debemos intercambiar la $q$-ésima fila por una cualquiera de las siguientes, por ejemplo la fila $k$, en la que $a_{kq} \neq 0, k>q$.
- Esta estrategia para hallar un pivote no nulo se llama **pivoteo trivial**.
- Ejemplo propuesto:

$$
\begin{cases} 
x-2y+z=-4 \\
-2x+4y-3z=3 \\
x-3y-4z=-1
\end{cases}
$$

# Estrategias de pivoteo para reducir los errores

- Como ya sabemos, dado que las computadoras usan una aritmética cuya precisión está fijada de antemano, es posible que cada vez que se realice una operación aritmética se introduzca un pequeño error.
- En la resolución de ecuaciones por eliminación gaussiana, un adecuado reordenamiento de las filas en el momento indicado puede reducir notablemente los errores cometidos. 
- Por ejemplo, se puede mostrar cómo buscar un multiplicador de menor magnitud (es decir, un pivote de mayor magnitud) mejora la precisión de los resultados. 
- Por eso, existen estrategias de pivoteo que no solamente hacen intercambio de filas cuando se tiene un pivote nulo, si no también cuando alguna de las filas posteriores tiene un pivote de mayor valor absoluto.

# Estrategias de pivoteo para reducir los errores: pivoteo parcial

- Para reducir la propagación de los errores de redondeo, antes de comenzar una nueva ronda de reemplazos con el pivote $a_{qq}$ se evalúa si debajo en la misma columna hay algún elemento con mayor valor absoluto y en ese caso se intercambian las respectivas filas.
- Es decir, se busca si existe $r$ tal que $|a_{rq}| > |a_{qq}|,\quad r>q$ para luego intercambiar las filas $q$ y $r$.
- Este proceso suele conservar las magnitudes relativas de los elementos de la matriz triangular superior en el mismo orden que las de los coeficientes de la matriz original.

# Estrategias de pivoteo para reducir los errores: pivoteo parcial escalado

- Reduce aún más los efectos de la propagación de los errores.
- Se elige el elemento de la columna $q$-ésima, en o por debajo de la diagonal principal, que tiene mayor tamaño relativo con respecto al resto de los elementos de su fila.
- **Paso 1**: buscar el máximo valor absoluto en cada fila:

$$
s_r = max\{|a_{rq}|, |a_{r,q+1}|, \cdots, |a_{rn}| \} \quad r = q, q+1, \cdots, n
$$

- **Paso 2**: elegir como fila pivote a la que tenga el mayor valor de $\frac{|a_{rq}|}{s_r}$, $r = q, q+1, \cdots, n$.
- **Paso 3**: intercambiar la fila $q$ con la fila hallada en el paso 2.

# Método de eliminación de Gauss-Jordan

- Ya vimos que el método de Gauss transforma la matriz de coeficientes en una matriz triangular superior. 
- El método de Gauss-Jordan continúa el proceso de transformación hasta obtener la matriz identidad.
- **Retomar Ejemplo Diapo 15**: cuando aplicamos eliminación de Gauss llegamos a la siguiente matriz triangular.

$$
\begin{bmatrix}
1 & 2 & -1 & 3 &\aug& -8\\
0 & -4 & 4 & -7 &\aug& 29\\
0 & 0 & -12 & 13 &\aug& -87\\
0 & 0 & 0 & -136 &\aug& 408  
\end{bmatrix}
$$

- Con el método de sustitución regresiva, llegamos a encontrar $x=1$, $y=2$, $z=4$ y $t=-3$.

# Método de eliminación de Gauss-Jordan

- En lugar de aplicar sustitución regresiva, seguimos operando por fila hasta lograr una matriz diagonal:

$$
\begin{array}{cl}
\text{Fila 2} &= \text{ Fila 2} / (-4) \\ 
\text{Fila 3} &= \text{ Fila 3} /(-12) \\
\text{Fila 4} &= \text{ Fila 4} / (-136) 
\end{array}
\implies
\begin{bmatrix}
1 & 2 & -1 & 3 &\aug& -8\\
0 & 1 & -1 & 7/4 &\aug& -29/4\\
0 & 0 & 1 & -13/12 &\aug& 87/12\\
0 & 0 & 0 & 1 &\aug& -3  
\end{bmatrix}
$$

$$
\begin{array}{cl}
\text{Fila 1} &= \text{ Fila 1} - 3 \text{ Fila 4} \\ 
\text{Fila 2} &= \text{ Fila 2} - 7/4 \text{ Fila 4}\\
\text{Fila 3} &= \text{ Fila 3} + 13/12 \text{ Fila 4} 
\end{array}
\implies
\begin{bmatrix}
1 & 2 & -1 & 0 &\aug& 1\\
0 & 1 & -1 & 0 &\aug& -2\\
0 & 0 & 1 & 0 &\aug& 4\\
0 & 0 & 0 & 1 &\aug& -3  
\end{bmatrix}
$$

# Método de eliminación de Gauss-Jordan

$$
\begin{array}{cl}
\text{Fila 1} &= \text{ Fila 1} + \text{ Fila 3} \\ 
\text{Fila 2} &= \text{ Fila 2} + \text{ Fila 3}
\end{array}
\implies
\begin{bmatrix}
1 & 2 & 0 & 0 &\aug& 5\\
0 & 1 & 0 & 0 &\aug& 2\\
0 & 0 & 1 & 0 &\aug& 4\\
0 & 0 & 0 & 1 &\aug& -3  
\end{bmatrix}
$$

$$
\begin{array}{cl}
\text{Fila 1} &= \text{ Fila 1} -2 \text{ Fila 2} 
\end{array}
\implies
\begin{bmatrix}
1 & 0 & 0 & 0 &\aug& 1\\
0 & 1 & 0 & 0 &\aug& 2\\
0 & 0 & 1 & 0 &\aug& 4\\
0 & 0 & 0 & 1 &\aug& -3  
\end{bmatrix}
$$

$$
\implies
\left\{
\begin{aligned}
x &= 1\\ y &= 2\\ z &= 4\\ t &= -3
\end{aligned}
\right.
$$

# Método de eliminación de Gauss-Jordan

- La ventaja de este método es que permite también hallar fácilmente la matriz inversa de $\mathbf{A}$.
- Para esto hay que concatenar a la derecha de la matriz aumentada una matriz identidad de orden $n$. Cuando en la submatriz izquierda se llega a la matriz identidad, en el centro habrá quedado el vector solución y a su derecha la matriz inversa.
- Ejemplo. Resolver el siguiente sistema y hallar la inversa de la matriz de coeficientes:

$$
\begin{cases} 
x-y+z=-4 \\
5x-4y+3z=-12 \\
2x+y+z=11
\end{cases}
$$

# Método de eliminación de Gauss-Jordan

$$
\begin{small}
\begin{bmatrix}
	1 & -1 & 1 &\aug& -4 &\aug& 1 & 0 & 0\\
	5 & -4 & 3 &\aug& -12 &\aug& 0 & 1 & 0\\
	2 & 1 & 1 &\aug& 11 &\aug& 0 & 0 & 1
\end{bmatrix}
\end{small}
$$
$$
\begin{tiny}
\begin{array}{cl}
	F2 &= F2 - 5 F1 \\
	F3 &= F3 - 2 F1 \\
	F1 &= F1 + F2 \\
	F3 &= F3 - 3 F2 \\
	F1 &= F1 + 1/5 F3 \\
	F2 &= F2 + 2/5 F3  \\
	F3 &= F3 / 5
\end{array}
\end{tiny}
$$
$$
\begin{small}
\begin{bmatrix}
	1 & 0 & 0 &\aug& 3 &\aug& -7/5 & 2/5 & 1/5\\
	0 & 1 & 0 &\aug& 6 &\aug& 1/5 & -1/5 & 2/5\\
	0 & 0 & 1 &\aug& -1 &\aug& 13/5 & -3/2 & 1/5
\end{bmatrix}
\end{small}
$$

# Método de eliminación de Gauss-Jordan

- ¿Por qué este procedimiento nos devuelva la inversa de la matriz de coeficientes?
- Recordar que la inversa de $\mathbf{A}$ es aquella matriz $\mathbf{A}^{-1}$ que verifica $\mathbf{AA}^{-1} =\mathbf{A}^{-1}\mathbf{A}=\mathbf{I}$
- Entonces queremos hallar la matriz $\mathbf{A}^{-1}$:

    \setlength{\abovedisplayshortskip}{-10pt}
    \setlength{\belowdisplayshortskip}{-10pt}
    $$
    \mathbf{A}^{-1} =
    \begin{bmatrix}
    x_{11} & x_{12} & x_{13} \\
    x_{21} & x_{22} & x_{23} \\
    x_{31} & x_{32} & x_{33} 
    \end{bmatrix}
    $$

    tal que:
    
    $$
    \begin{bmatrix}
    1 & -1 & 1 \\
    5 & -4 & 3 \\
    2 & 1 & 1 
    \end{bmatrix}
    \times
    \begin{bmatrix}
    x_{11} & x_{12} & x_{13} \\
    x_{21} & x_{22} & x_{23} \\
    x_{31} & x_{32} & x_{33} 
    \end{bmatrix}
    =
    \begin{bmatrix}
    1 & 0 & 0 \\
    0 & 1 & 0 \\
    0 & 0 & 1 
    \end{bmatrix}
    $$

# Método de eliminación de Gauss-Jordan

- Esto da lugar a tres sistemas de ecuaciones con la misma matriz de coeficientes:

$$
\begin{small}
\implies
\begin{bmatrix}
x_{11}-x_{21}+x_{31} & x_{12}-x_{22}+x_{32} & x_{13}-x_{23}+x_{33} \\
5x_{11}-4x_{21}+3x_{31} & 5x_{12}-4x_{22}+3x_{32} & 5x_{13}-4x_{23}+3x_{33} \\
2x_{11}+x_{21}+x_{31} & 2x_{12}-x_{22}+x_{32} & 2x_{13}-x_{23}+x_{33} 
\end{bmatrix}
=
\begin{bmatrix}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 1 
\end{bmatrix}
\end{small}
$$

$$
\begin{small}
\begin{cases} 
x_{11}-x_{21}+x_{31}=1 \\
5x_{11}-4x_{21}+3x_{31}=0 \\
2x_{11}+x_{21}+x_{31}=0
\end{cases}
\begin{cases} 
x_{12}-x_{22}+x_{32}=0 \\
5x_{12}-4x_{22}+3x_{32}=1 \\
2x_{12}+x_{22}+x_{32}=0
\end{cases}
\begin{cases} 
x_{13}-x_{23}+x_{33}=0 \\
5x_{13}-4x_{23}+3x_{33}=0 \\
2x_{13}+x_{23}+x_{33}=1
\end{cases}
\end{small}
$$

- Los resolvemos simultáneamente al agregar la matriz identidad de orden $n$ a la derecha de la matriz aumentada del sistema original.

# Métodos Aproximados o Iterativos

- **Objetivo**: extender a espacios de dimensión mayor que uno las ideas de los métodos iterativos vistos en la unidad 2 para resolver sistemas de ecuaciones lineales.
- Veremos:
    
    - Método de Jacobi
    - Método de Gauss-Seidel
    
# Método de Jacobi

- Consideremos el sistema: 

\setlength{\abovedisplayshortskip}{-10pt}
\setlength{\belowdisplayshortskip}{-10pt}
$$
\begin{cases} 
4x-y+z=7 \\
4x-8y+z=-21 \\
-2x+y+5z=15
\end{cases}
\implies
\begin{cases} 
x=(7+y-z)/4 \\
y=(21+4x+z)/8 \\
z=(15+2x-y)/5
\end{cases}
$$

- Despejar una incógnita en cada ecuación provee expresiones que sugieren la idea de un proceso iterativo.
- Dado un vector de valores iniciales $\mathbf{x_0}=(x_0, y_0, z_0)$, operar con la siguiente fórmula de recurrencia hasta la convergencia:

\setlength{\abovedisplayshortskip}{-10pt}
\setlength{\belowdisplayshortskip}{-10pt}
$$
\begin{cases} 
x_{k+1}=(7+y_k-z_k)/4 \\
y_{k+1}=(21+4x_k+z_k)/8 \\
z_{k+1}=(15+2x_k-y_k)/5
\end{cases}
$$

# Método de Jacobi

- Por ejemplo, tomando el valor inicial $(1, 2, 2)$, el proceso converge hacia la solución exacta del sistema $(2, 4, 3)$.
- Usando 4 posiciones decimales con redondeo luego de la coma:

\begin{table}[]
%\vspace{-\baselineskip}
\centering
\small
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{$k$} & \textbf{$x_k$} & \textbf{$y_k$} & \textbf{$z_k$} \\ \midrule
0            & 1              & 2              & 2              \\
1            & 1.7500         & 3.3750         & 3.0000         \\
2            & 1.8438         & 3.8750         & 3.0250         \\
3            & 1.9625         & 3.9250         & 2.9625         \\
4            & 1.9906         & 3.9766         & 3.0000         \\
5            & 1.9942         & 3.9953         & 3.0009         \\
6            & 1.9986         & 3.9972         & 2.9986         \\
7            & 1.9997         & 3.9991         & 3.0000         \\
8            & 1.9998         & 3.9999         & 3.0001         \\
9            & 2.0000         & 3.9999         & 2.9999         \\
10           & 2.0000         & 4.0000         & 3.0000      \\
\bottomrule
\end{tabular}
\end{table}

# Método de Jacobi

- *Observación*: no siempre este método converge. Es sensible al ordenamiento de las ecuaciones dentro del sistema.
- Ejemplo: tomamos el mismo sistema de antes pero intercambiamos las filas 1 y 3:

\setlength{\abovedisplayshortskip}{-10pt}
\setlength{\belowdisplayshortskip}{-10pt}
$$
\begin{cases} 
4x-y+z=7 \\
4x-8y+z=-21 \\
-2x+y+5z=15
\end{cases}
\implies
\begin{cases} 
-2x+y+5z=15 \\
4x-8y+z=-21 \\
4x-y+z=7
\end{cases}
$$

$$
\implies
\begin{cases} 
x=(-15+y+5z)/2 \\
y=(21+4x+z)/8 \\
z=7-4x+y
\end{cases}
\implies
\begin{cases} 
x_{k+1}=(-15+y_k+5z_k)/2 \\
y_{k+1}=(21+4x_k+z_k)/8 \\
z_{k+1}=7-4x_k+y_k
\end{cases}
$$

# Método de Jacobi

- Tomando el mismo valor inicial $(1, 2, 2)$, esta vez el proceso diverge:

\begin{table}[]
%\vspace{-\baselineskip}
\centering
\small
\begin{tabular}{@{}cccc@{}}
\toprule
\textbf{$k$} & \textbf{$x_k$} & \textbf{$y_k$} & \textbf{$z_k$} \\ \midrule
0            & 1              & 2              & 2              \\
1            & -1.5000        & 3.3750         & 5.0000         \\
2            & 6.6875         & 2.5000         & 16.3750        \\
3            & 34.6875        & 8.0156         & -17.2500       \\
4            & -46.6172       & 17.8125        & -123.7344      \\
5            & -307.9298      & -36.1504       & 211.2813       \\
6            & 502.6281       & -124.9297      & 1202.5688      \\
7            & 2936.4572      & 404.2602       & -2128.4421     \\
8            & -5126.4752     & 1204.7983      & -11334.5686    \\
9            & -27741.5224    & -3977.4337     & 21717.6991     \\
10           & 52298.0309     & -11153.4238    & 106995.6559    \\
\bottomrule
\end{tabular}
\end{table}

# Método de Gauss-Seidel

- Toma la misma idea que Jacobi, pero con una pequeña modificación para acelerar la convergencia. Retomando el ejemplo inicial:

\setlength{\abovedisplayshortskip}{-20pt}
$$
\begin{cases} 
4x-y+z=7 \\
4x-8y+z=-21 \\
-2x+y+5z=15
\end{cases}
\implies
\begin{cases} 
x=(7+y-z)/4 \\
y=(21+4x+z)/8 \\
z=(15+2x-y)/5
\end{cases}
$$

**Jacobi:** \hspace{4.5cm} **Gauss-Seidel:**

\setlength{\abovedisplayshortskip}{-40pt}
\setlength{\belowdisplayshortskip}{-20pt}
\vspace{-1cm}
$$
\begin{cases} 
x_{k+1}=(7+y_k-z_k)/4 \\
y_{k+1}=(21+4x_k+z_k)/8 \\
z_{k+1}=(15+2x_k-y_k)/5
\end{cases}
\quad \quad
\begin{cases} 
x_{k+1}=(7+y_k-z_k)/4 \\
y_{k+1}=(21+4x_{k+1}+z_k)/8 \\
z_{k+1}=(15+2x_{k+1}-y_{k+1})/5
\end{cases}
$$

- La diferencia está en que apenas calcula un nuevo valor de las incógnitas, Gauss-Seidel lo usa inmediatamente en el cálculo de las restantes, en lugar esperar a la próxima ronda.

# Método de Gauss-Seidel

- Tomando otra vez el valor inicial $(1, 2, 2)$ y operando con 4 posiciones decimales con redondeo luego de la coma:

\begin{table}[]
%\vspace{-\baselineskip}
\centering
\small
\begin{tabular}{@{}cccc|cccc@{}}
\toprule
\multicolumn{4}{c|}{\textbf{Jacobi}}                            & \multicolumn{4}{c}{\textbf{Gauss-Seidel}}                       \\ \midrule
\textbf{$k$} & \textbf{$x_k$} & \textbf{$y_k$} & \textbf{$z_k$} & \textbf{$k$} & \textbf{$x_k$} & \textbf{$y_k$} & \textbf{$z_k$} \\ \midrule
0            & 1              & 2              & 2              & 0            & 1              & 2              & 2              \\
1            & 1.7500         & 3.3750         & 3.0000         & 1            & 1.7500         & 3.7500         & 2.9500         \\
2            & 1.8438         & 3.8750         & 3.0250         & 2            & 1.9500         & 3.9688         & 2.9862         \\
3            & 1.9625         & 3.9250         & 2.9625         & 3            & 1.9957         & 3.9961         & 2.9991         \\
4            & 1.9906         & 3.9766         & 3.0000         & 4            & 1.9993         & 3.9995         & 2.9998         \\
5            & 1.9942         & 3.9953         & 3.0009         & 5            & 1.9999         & 3.9999         & 3.0000         \\
6            & 1.9986         & 3.9972         & 2.9986         & 6            & 2.0000         & 4.0000         & 3.0000         \\
7            & 1.9997         & 3.9991         & 3.0000         &              &                &                &                \\
8            & 1.9998         & 3.9999         & 3.0001         &              &                &                &                \\
9            & 2.0000         & 3.9999         & 2.9999         &              &                &                &                \\
10           & 2.0000         & 4.0000         & 3.0000         &              &                &                &                \\ \bottomrule
\end{tabular}
\end{table}

# Generalización

- Ahora que sabemos qué hace cada método y en qué se diferencian vamos a:
    
    - Generalizar las fórmulas recursivas
    - Escribirlas matricialmente
    - Evaluar la convergencia
    - Ver los programas

# Generalización

- Tenemos un sistema de $n$ ecuaciones lineales con $n$ incógnitas y despejamos una variable en cada ecuación:

$$
\begin{small}
%\hspace{-0.4cm}
\begin{cases} 
a_{11}x_1 + a_{12}x_2 + \cdots + a_{1n}x_n = b_1 \\
a_{21}x_1 + a_{22}x_2 + \cdots + a_{2n}x_n = b_2 \\
\vdots \\
a_{j1}x_1 + a_{j2}x_2 + \cdots + a_{jn}x_n = b_j \\
\vdots \\
a_{n1}x_1 + a_{n2}x_2 + \cdots + a_{nn}x_n = b_n
\end{cases}
\hspace{-0.6cm}\implies\hspace{-0.2cm}
\begin{cases} 
x_1 = \frac{b_1 - a_{12}x_2 - \cdots - a_{1n}x_n}{ a_{11}}\\
x_2 = \frac{b_2 - a_{21}x_1 - a_{23}x_3 - \cdots - a_{2n}x_n}{ a_{22}}\\
\vdots \\
x_j = \frac{b_j - a_{j1}x_1 \cdots - a_{j(j-1)}x_{j-1} - a_{j(j+1)}x_{j+1} - \cdots - a_{jn}x_n}{ a_{jj}}\\
\vdots \\
x_n = \frac{b_n - a_{n1}x_1 - \cdots - a_{n(n-1)}x_{n-1}}{a_{nn}}\\
\end{cases}
\end{small}
$$

# Generalización del Método de Jacobi

- Con los supra índices indicando el número de iteración, la fórmula de recurrencia es:

\vspace{-0.75cm}
$$
\begin{cases} 
x_1^{(k+1)} = \frac{b_1 - a_{12}x_2^{(k)} - \cdots - a_{1n}x_n^{(k)}}{ a_{11}}\\
x_2^{(k+1)} = \frac{b_2 - a_{21}x_1^{(k)} - a_{23}x_3^{(k)} - \cdots - a_{2n}x_n^{(k)}}{ a_{22}}\\
\vdots \\
x_j^{(k+1)} = \frac{b_j - a_{j1}x_1^{(k)} \cdots - a_{j(j-1)}x_{j-1}^{(k)} - a_{j(j+1)}x_{j+1}^{(k)} - \cdots - a_{jn}x_n^{(k)}}{ a_{jj}}\\
\vdots \\
x_n^{(k+1)} = \frac{b_n - a_{n1}x_1^{(k)} - \cdots - a_{n(n-1)}x_{n-1}^{(k)}}{a_{nn}}\\
\end{cases}
$$

$$
\implies
x_j^{(k+1)} = \frac{b_j - a_{j1}x_1^{(k)} \cdots - a_{j(j-1)}x_{j-1}^{(k)} - a_{j(j+1)}x_{j+1}^{(k)} - \cdots - a_{jn}x_n^{(k)}}{ a_{jj}}
$$
$$
k=1, 2, \cdots n
$$

# Generalización del Método de Gauss-Seidel

- Con los supra índices indicando el número de iteración, la fórmula de recurrencia es:

\vspace{-0.75cm}
$$
\begin{cases} 
x_1^{(k+1)} = \frac{b_1 - a_{12}x_2^{(k)} - \cdots - a_{1n}x_n^{(k)}}{ a_{11}}\\
x_2^{(k+1)} = \frac{b_2 - a_{21}x_1^{(k+1)} - a_{23}x_3^{(k)} - \cdots - a_{2n}x_n^{(k)}}{ a_{22}}\\
\vdots \\
x_j^{(k+1)} = \frac{b_j - a_{j1}x_1^{(k+1)} \cdots - a_{j(j-1)}x_{j-1}^{(k+1)} - a_{j(j+1)}x_{j+1}^{(k)} - \cdots - a_{jn}x_n^{(k)}}{ a_{jj}}\\
\vdots \\
x_n^{(k+1)} = \frac{b_n - a_{n1}x_1^{(k+1)} - \cdots - a_{n(n-1)}x_{n-1}^{(k+1)}}{a_{nn}}\\
\end{cases}
$$

$$
\implies
x_j^{(k+1)} = \frac{b_j - a_{j1}x_1^{(k+1)} \cdots - a_{j(j-1)}x_{j-1}^{(k+1)} - a_{j(j+1)}x_{j+1}^{(k)} - \cdots - a_{jn}x_n^{(k)}}{ a_{jj}}
$$
$$
k=1, 2, \cdots n
$$

# Expresión matricial para el Método de Jacobi

- Si descomponemos a la matriz $\mathbf{A}$ como $\mathbf{A=D+R}$, donde $\mathbf{D}$ es la matriz diagonal formada con la diagonal de $\mathbf{A}$ y $\mathbf{R}$ es igual a $\mathbf{A}$ excepto en la diagonal donde posee todos ceros, tenemos:

\vspace{-1cm}
\begin{align*} 
\mathbf{Ax} &= \mathbf{b} \\
\mathbf{(D+R)x} &= \mathbf{b} \\
\mathbf{Dx} &= \mathbf{b} - \mathbf{Rx}  \\
\mathbf{x} &= \mathbf{D}^{-1} (\mathbf{b} - \mathbf{Rx})  \\
\end{align*}
\vspace{-1cm}

- Esto da lugar a la siguiente fórmula de recurrencia, que es equivalente a las vistas anteriormente, y facilita la programación del método:

\vspace{-0.5cm}
$$
\mathbf{x}^{(k+1)} = \mathbf{D}^{-1} (\mathbf{b} - \mathbf{Rx}^{(k)}) 
$$
\vspace{-1cm}

- Requisito: ningún elemento en la diagonal de $\mathbf{A}$ es cero.

# Expresión matricial para el Método de Gauss-Seidel

- Si descomponemos a la matriz $\mathbf{A}$ como $\mathbf{A=L+U}$, donde $\mathbf{L}$ es una matriz triangular inferior (incluyendo la diagonal de $\mathbf{A}$) y $\mathbf{U}$ es una matriz triangular superior (con ceros en la diagonal), tenemos:

\vspace{-1cm}
\begin{align*} 
\mathbf{Ax} &= \mathbf{b} \\
\mathbf{(L+U)x} &= \mathbf{b} \\
\mathbf{Lx} &= \mathbf{b} - \mathbf{Ux}  \\
\mathbf{x} &= \mathbf{L}^{-1} (\mathbf{b} - \mathbf{Ux})  \\
\end{align*}
\vspace{-1cm}

- Esto da lugar a la siguiente fórmula de recurrencia, que es equivalente a las vistas anteriormente, y facilita la programación del método:

\vspace{-0.5cm}
$$
\mathbf{x}^{(k+1)} = \mathbf{L}^{-1} (\mathbf{b} - \mathbf{Ux}^{(k)}) 
$$
\vspace{-1cm}

- Requisito: ningún elemento en la diagonal de $\mathbf{A}$ es cero.

# Convergencia

## Definición

Se dice que una matriz $\mathbf{A}$ de orden $n \times n$ es **diagonal estrictamente dominante** cuando el elemento diagonal es mayor a la suma del resto de los elementos de su fila en valor absoluto:

\setlength{\abovedisplayshortskip}{-10pt}
\setlength{\belowdisplayshortskip}{-5pt}
$$
\begin{small}
|a_{kk}| > \sum\limits_{\substack{j=0 \\ j\neq k}}^n |a_{kj}| \quad k=1, 2, \cdots, n
\end{small}
$$

\vspace{-0.01cm}

## Teorema

Si la matriz $\mathbf{A}$ es diagonal estrictamente dominante, entonces el sistema lineal $\mathbf{Ax=b}$ tiene solución única y los procesos iterativos de Jacobi y de Gauss-Seidel convergen hacia la misma cualquiera sea el vector de partida $\mathbf{x_0}$.

\vspace{-0.1cm}

##
- Es una condición suficiente pero no necesaria.

# Convergencia

**Criterios para la convergencia**

- **Norma L1 (distancia Manhattan)**:

\vspace{-0.5cm}
$$
||\mathbf{x}_{k+1}-\mathbf{x}_{k}||_1 =\sum_{j=1}^n |x_j^{(k+1)} - x_j^{(k)}| < \epsilon
$$

\vspace{-0.5cm}

- **Norma L2 (norma euclídea)**:

\vspace{-0.5cm}
$$||\mathbf{x}_{k+1}-\mathbf{x}_{k}||_2 = \sqrt{(\mathbf{x}_{k+1}-\mathbf{x}_{k})'(\mathbf{x}_{k+1}-\mathbf{x}_{k})} = \sqrt{\sum_{j=1}^n (x_j^{(k+1)} - x_j^{(k)})^2} < \epsilon$$

\vspace{-0.5cm}

- **Norma L$_\infty$ (máxima diferencia)**:

\vspace{-0.5cm}
$$
||\mathbf{x}_{k+1}-\mathbf{x}_{k}||_\infty = \max_{j} |x_j^{(k+1)} - x_j^{(k)}| < \epsilon
$$
\vspace{-0.5cm}

- **Establecer un número máximo de iteraciones**

# Ventajas de Gauss-Seidel

&nbsp;

&nbsp;

- Converge más rápidamente.
- Puede converger cuando Jacobi no lo hace (aunque $\mathbf{A}$ no sea diagonal estrictamente dominante, por ejemplo si $\mathbf{A}$ es semidefinida positiva).
- Pero puede haber casos en los que Jacobi converge y Gauss-Seidel no.

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

&nbsp;

```{r, echo = T}
":)"
```

